# üöÄ H∆∞·ªõng D·∫´n Nhanh - Fine-tune LLM M√¥i Tr∆∞·ªùng

## B∆∞·ªõc 1: C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng

```bash
# T·∫°o conda environment
conda create -n env-llm python=3.10
conda activate env-llm

# C√†i ƒë·∫∑t PyTorch v·ªõi CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt
```

## B∆∞·ªõc 2: C·∫•u h√¨nh

```bash
cp .env.example .env
```

Th√™m v√†o `.env`:
```
HF_TOKEN=hf_your_huggingface_token  # N·∫øu d√πng Llama
WANDB_API_KEY=your_wandb_key        # Optional
```

## B∆∞·ªõc 3: Chu·∫©n b·ªã d·ªØ li·ªáu

```bash
# T·∫°o dataset m·∫´u
python scripts/prepare_data.py \
    --generate_sample \
    --output data/processed/climate_qa.json \
    --split

# Ho·∫∑c t·ª´ file c√≥ s·∫µn
python scripts/prepare_data.py \
    --input data/raw/your_data.json \
    --output data/processed/climate_qa.json \
    --format qa \
    --split
```

## B∆∞·ªõc 4: Fine-tune

### Option A: LoRA (GPU 16GB+)
```bash
python scripts/train.py \
    --model_name microsoft/phi-2 \
    --dataset data/processed/climate_qa_train.json \
    --output_dir models/phi2-climate-lora \
    --config configs/lora_config.yaml \
    --epochs 3
```

### Option B: QLoRA (GPU 8GB)
```bash
python scripts/train.py \
    --model_name microsoft/phi-2 \
    --dataset data/processed/climate_qa_train.json \
    --output_dir models/phi2-climate-qlora \
    --config configs/qlora_config.yaml \
    --use_qlora \
    --epochs 3
```

## B∆∞·ªõc 5: ƒê√°nh gi√°

```bash
python scripts/evaluate.py \
    --model_path models/phi2-climate-lora \
    --base_model microsoft/phi-2 \
    --test_data data/processed/climate_qa_test.json
```

## B∆∞·ªõc 6: S·ª≠ d·ª•ng

```python
from src.inference import EnvironmentalLLM

# Load model
llm = EnvironmentalLLM(
    model_path="models/phi2-climate-lora",
    base_model="microsoft/phi-2"
)

# Generate
response = llm.generate("What is climate change?")
print(response)
```

## üìä So s√°nh LoRA vs QLoRA

| Aspect | LoRA | QLoRA |
|--------|------|-------|
| **VRAM c·∫ßn thi·∫øt** | 16GB+ | 8GB |
| **T·ªëc ƒë·ªô training** | Nhanh h∆°n | Ch·∫≠m h∆°n ~20% |
| **Ch·∫•t l∆∞·ª£ng** | T·ªët nh·∫•t | G·∫ßn b·∫±ng LoRA |
| **Models ph√π h·ª£p** | 7B-13B | 7B-13B (quantized) |

## üîß C√°c tham s·ªë quan tr·ªçng

### LoRA Config
- `r`: Rank (8-64, cao h∆°n = nhi·ªÅu params h∆°n)
- `lora_alpha`: Scaling factor (th∆∞·ªùng = 2*r)
- `target_modules`: Layers ƒë·ªÉ fine-tune

### Training Config
- `batch_size`: ƒêi·ªÅu ch·ªânh theo VRAM
- `learning_rate`: 1e-4 ƒë·∫øn 3e-4
- `epochs`: 2-5 cho datasets nh·ªè

## ‚ùì Troubleshooting

### "CUDA out of memory"
- Gi·∫£m `batch_size`
- D√πng QLoRA thay LoRA
- B·∫≠t `gradient_checkpointing`

### "Model not loading"
- Ki·ªÉm tra HF_TOKEN cho gated models
- Ki·ªÉm tra internet connection

### Training kh√¥ng converge
- Gi·∫£m learning_rate
- TƒÉng epochs
- Ki·ªÉm tra data quality

---

**üéØ Portfolio Project #4** - Environmental Domain LLM Fine-tuning
