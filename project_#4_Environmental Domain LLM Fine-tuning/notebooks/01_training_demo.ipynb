{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Environmental LLM Fine-tuning Demo\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model for environmental domain tasks using LoRA/QLoRA.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Installation\n",
    "2. Data Preparation\n",
    "3. Model Loading\n",
    "4. LoRA Configuration\n",
    "5. Training\n",
    "6. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install torch transformers peft accelerate bitsandbytes datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "data_path = Path('../data/raw/climate_qa_comprehensive.json')\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} examples\")\n",
    "print(\"\\nSample example:\")\n",
    "print(json.dumps(data[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import print_dataset_stats\n",
    "\n",
    "# Print statistics\n",
    "print_dataset_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import format_instruction\n",
    "\n",
    "# Format an example using Alpaca template\n",
    "example = data[0]\n",
    "formatted = format_instruction(\n",
    "    instruction=example['instruction'],\n",
    "    input_text=example.get('input', ''),\n",
    "    output=example['output'],\n",
    "    template_name='alpaca'\n",
    ")\n",
    "\n",
    "print(\"Formatted example:\")\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_utils import load_model, load_tokenizer\n",
    "\n",
    "# Choose a small model for demo\n",
    "MODEL_NAME = \"microsoft/phi-2\"  # 2.7B parameters\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = load_tokenizer(MODEL_NAME)\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPU with 8GB VRAM, use QLoRA\n",
    "USE_QLORA = True\n",
    "\n",
    "if USE_QLORA:\n",
    "    from src.config import QuantizationConfig\n",
    "    \n",
    "    quant_config = QuantizationConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = load_model(\n",
    "        model_name=MODEL_NAME,\n",
    "        quantization_config=quant_config,\n",
    "    )\n",
    "else:\n",
    "    model = load_model(\n",
    "        model_name=MODEL_NAME,\n",
    "        torch_dtype=\"float16\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import LoRAConfig\n",
    "from src.model_utils import apply_lora\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoRAConfig(\n",
    "    r=16,              # Rank\n",
    "    lora_alpha=32,     # Scaling factor\n",
    "    lora_dropout=0.05, # Dropout\n",
    "    target_modules=[   # Modules to adapt\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = apply_lora(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import create_hf_dataset\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "datasets = create_hf_dataset(\n",
    "    data=data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,  # Shorter for demo\n",
    "    template_name=\"alpaca\",\n",
    "    train_split=0.9,\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(datasets['train'])}\")\n",
    "print(f\"Eval samples: {len(datasets['eval'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import TrainingConfig\n",
    "from src.trainer import EnvironmentalTrainer\n",
    "\n",
    "# Training configuration (reduced for demo)\n",
    "training_config = TrainingConfig(\n",
    "    output_dir=\"../models/demo_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = EnvironmentalTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=datasets['train'],\n",
    "    eval_dataset=datasets['eval'],\n",
    "    training_config=training_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (uncomment to run)\n",
    "# result = trainer.train()\n",
    "# print(f\"Training loss: {result['training_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with base model (before training)\n",
    "model.eval()\n",
    "\n",
    "def generate_response(instruction, max_tokens=128):\n",
    "    prompt = format_instruction(\n",
    "        instruction=instruction,\n",
    "        template_name=\"alpaca\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is climate change?\",\n",
    "    \"How do electric vehicles help the environment?\",\n",
    "    \"What is ESG investing?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"Q: {question}\")\n",
    "    response = generate_response(question)\n",
    "    print(f\"A: {response}\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Full Training**: Run complete training with more epochs\n",
    "2. **Evaluation**: Test on benchmark questions\n",
    "3. **Merge Weights**: Merge LoRA into base model\n",
    "4. **Deploy**: Export to GGUF or serve via API\n",
    "\n",
    "See `scripts/train.py` for full training workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
